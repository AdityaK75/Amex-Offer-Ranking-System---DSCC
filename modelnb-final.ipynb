{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12569280,"sourceType":"datasetVersion","datasetId":7903092}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e3a6b0cc","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:25:05.524319Z","iopub.execute_input":"2025-07-26T17:25:05.525094Z","iopub.status.idle":"2025-07-26T17:25:09.094706Z","shell.execute_reply.started":"2025-07-26T17:25:05.525057Z","shell.execute_reply":"2025-07-26T17:25:09.093974Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/val_temporal.csv\n/kaggle/input/train_temporal.csv\n/kaggle/input/test_features_final.csv\n/kaggle/input/test_features_final_r3.csv\n/kaggle/input/train_features_final.csv\n/kaggle/input/final_feature_names.csv\n","output_type":"stream"}],"execution_count":1},{"id":"cc581b29","cell_type":"code","source":"train_features_final=pd.read_csv('/kaggle/input/train_features_final.csv')\ntest_features_final=pd.read_csv('/kaggle/input/test_features_final.csv')\nfinal_feature_names=pd.read_csv('/kaggle/input/final_feature_names.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:25:09.095975Z","iopub.execute_input":"2025-07-26T17:25:09.096305Z","iopub.status.idle":"2025-07-26T17:26:23.998315Z","shell.execute_reply.started":"2025-07-26T17:25:09.096283Z","shell.execute_reply":"2025-07-26T17:26:23.997400Z"}},"outputs":[],"execution_count":2},{"id":"82b29b00-fa23-461a-9424-eef28b7b4973","cell_type":"code","source":"# Total popularity of an offer across all users\nglobal_offer_stats = train_features_final.groupby('id3').agg(\n    global_offer_impressions=('id1', 'count'),\n    global_offer_clicks=('y', 'sum')\n).reset_index()\n\nglobal_offer_stats['global_offer_ctr'] = global_offer_stats['global_offer_clicks'] / global_offer_stats['global_offer_impressions']\n\nbins = [0, 0.01, 0.03, 0.07, 0.15, 1]\nlabels = [0,1,2,3,4]\nglobal_offer_stats['offer_ctr_bucket'] = pd.cut(global_offer_stats['global_offer_ctr'], bins=bins, labels=labels, include_lowest=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:26:23.999688Z","iopub.execute_input":"2025-07-26T17:26:24.000677Z","iopub.status.idle":"2025-07-26T17:26:24.094606Z","shell.execute_reply.started":"2025-07-26T17:26:24.000654Z","shell.execute_reply":"2025-07-26T17:26:24.094055Z"}},"outputs":[],"execution_count":3},{"id":"b9d2f548-7c1f-4664-81c0-38664de16536","cell_type":"code","source":"train_features_final = train_features_final.merge(\n    global_offer_stats[['id3', 'offer_ctr_bucket']],\n    on='id3',\n    how='left'\n)\ntest_features_final = test_features_final.merge(\n    global_offer_stats[['id3', 'offer_ctr_bucket']],\n    on='id3',\n    how='left'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:26:24.095169Z","iopub.execute_input":"2025-07-26T17:26:24.095387Z","iopub.status.idle":"2025-07-26T17:26:25.524616Z","shell.execute_reply.started":"2025-07-26T17:26:24.095342Z","shell.execute_reply":"2025-07-26T17:26:25.523789Z"}},"outputs":[],"execution_count":4},{"id":"a0e869a7-6686-4e20-ab25-0852407a2a89","cell_type":"code","source":"# 1. click_rate_30d\ntrain_features_final['click_rate_30d'] = train_features_final['f206'] / (train_features_final['f207'] + 1e-5)\ntest_features_final['click_rate_30d'] = test_features_final['f206'] / (test_features_final['f207'] + 1e-5)\n\n# 4. engagement_score\ntrain_features_final['engagement_score'] = train_features_final['f137'] * train_features_final['f206']\ntest_features_final['engagement_score'] = test_features_final['f137'] * test_features_final['f206']\n\n# 5. log_ctr\ntrain_features_final['log_ctr_f137'] = np.log1p(train_features_final['f137'])\ntest_features_final['log_ctr_f137'] = np.log1p(test_features_final['f137'])\n\n# 6. exp_ctr\ntrain_features_final['exp_ctr_f134'] = np.exp(-train_features_final['f134'])\ntest_features_final['exp_ctr_f134'] = np.exp(-test_features_final['f134'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:26:25.526585Z","iopub.execute_input":"2025-07-26T17:26:25.526835Z","iopub.status.idle":"2025-07-26T17:26:25.584334Z","shell.execute_reply.started":"2025-07-26T17:26:25.526817Z","shell.execute_reply":"2025-07-26T17:26:25.583746Z"}},"outputs":[],"execution_count":5},{"id":"09b05b55-e43e-4874-90c3-31fe8a3b951b","cell_type":"code","source":"# Step 1: Map category one-hot to corresponding CTR feature\ncategory_ctr_map = { \n    'f227': 'f130',  # Dining\n    'f228': 'f131',  # Entertainment\n    'f230': 'f134',  # Services\n    'f231': 'f132',  # Shopping\n    'f232': 'f133',  # Travel\n}\n\n# Step 2: Create category_ctr by using the matching category's CTR\ndef get_category_ctr(row):\n    for cat_flag, ctr_col in category_ctr_map.items():\n        if row[cat_flag] == 1:\n            return row[ctr_col]\n    return np.nan  # fallback in case no match\n\ntrain_features_final['category_ctr'] = train_features_final.apply(get_category_ctr, axis=1)\ntest_features_final['category_ctr'] = test_features_final.apply(get_category_ctr, axis=1)\n\n# Drop NaNs\nplot_df = train_features_final[['category_ctr', 'y']].dropna()\n\n# Bin CTR for plotting clarity\nplot_df['ctr_bin'] = pd.cut(plot_df['category_ctr'], bins=10)\n\n# Grouped mean redemption rate per CTR bin\ngrouped = plot_df.groupby('ctr_bin')['y'].mean().reset_index()\ngrouped['ordinal_label'] = grouped['y'].rank(method='first').astype(int) - 1 \nprint(grouped['ordinal_label'])\n\ngrouped['interval'] = pd.IntervalIndex.from_tuples([\n    (-0.001, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 0.4), (0.4, 0.5),\n    (0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)\n])\nbin_to_label = dict(zip(grouped['interval'], grouped['ordinal_label']))\nbins = [-0.001, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\nbin_intervals = pd.cut(train_features_final['category_ctr'], bins=bins)\n\n# Step 4: Map to new label based on redemption rate\ntrain_features_final['category_ctr_bin'] = bin_intervals.map(bin_to_label).astype('Int64')\nbin_intervals = pd.cut(test_features_final['category_ctr'], bins=bins)\ntest_features_final['category_ctr_bin'] = bin_intervals.map(bin_to_label).astype('Int64')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:26:25.585122Z","iopub.execute_input":"2025-07-26T17:26:25.585440Z","iopub.status.idle":"2025-07-26T17:27:09.455696Z","shell.execute_reply.started":"2025-07-26T17:26:25.585391Z","shell.execute_reply":"2025-07-26T17:27:09.455016Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2004692735.py:27: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped = plot_df.groupby('ctr_bin')['y'].mean().reset_index()\n","output_type":"stream"},{"name":"stdout","text":"0    0\n1    5\n2    6\n3    8\n4    9\n5    7\n6    3\n7    4\n8    1\n9    2\nName: ordinal_label, dtype: int64\n","output_type":"stream"}],"execution_count":6},{"id":"1bd9ffc2-5a3c-44ea-8909-e2d849e79263","cell_type":"code","source":"train_features_final=train_features_final.drop(columns='category_ctr')\ntest_features_final=test_features_final.drop(columns='category_ctr')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:27:09.456547Z","iopub.execute_input":"2025-07-26T17:27:09.456806Z","iopub.status.idle":"2025-07-26T17:27:10.637388Z","shell.execute_reply.started":"2025-07-26T17:27:09.456774Z","shell.execute_reply":"2025-07-26T17:27:10.636770Z"}},"outputs":[],"execution_count":7},{"id":"ce61902e-f097-46d8-b13b-49ebe49951b8","cell_type":"code","source":"train_features_final[\"id5\"]=pd.to_datetime(train_features_final['id5'],errors='coerce')\ntest_features_final[\"id5\"]=pd.to_datetime(test_features_final['id5'],errors='coerce')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:27:10.638088Z","iopub.execute_input":"2025-07-26T17:27:10.638273Z","iopub.status.idle":"2025-07-26T17:27:10.768418Z","shell.execute_reply.started":"2025-07-26T17:27:10.638258Z","shell.execute_reply":"2025-07-26T17:27:10.767851Z"}},"outputs":[],"execution_count":8},{"id":"a27e609f-4d47-41be-865a-ac04d3597329","cell_type":"code","source":"train_features_final[\"minute\"]=train_features_final['id5'].dt.minute\ntest_features_final[\"minute\"]=test_features_final['id5'].dt.minute","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:27:10.769062Z","iopub.execute_input":"2025-07-26T17:27:10.769248Z","iopub.status.idle":"2025-07-26T17:27:10.807424Z","shell.execute_reply.started":"2025-07-26T17:27:10.769232Z","shell.execute_reply":"2025-07-26T17:27:10.806547Z"}},"outputs":[],"execution_count":9},{"id":"28bca371-2d86-4712-b123-0cb56a7da563","cell_type":"code","source":"print(train_features_final.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:27:10.808229Z","iopub.execute_input":"2025-07-26T17:27:10.808432Z","iopub.status.idle":"2025-07-26T17:27:10.813244Z","shell.execute_reply.started":"2025-07-26T17:27:10.808415Z","shell.execute_reply":"2025-07-26T17:27:10.812477Z"}},"outputs":[{"name":"stdout","text":"['id1', 'id2', 'id3', 'id4', 'id5', 'id10', 'id8', 'id12', 'id13', 'id6', 'id7', 'time_since_last_impression', 'user_session_length', 'f366', 'hourly_offer_frequency', 'user_cumulative_impressions', 'f132', 'f125', 'f207', 'f223', 'f363', 'f210', 'f314', 'f206', 'f358', 'user_avg_session_length', 'f38', 'f337', 'f26', 'f139', 'f151', 'f39', 'f212', 'f203', 'f343', 'f350', 'f95', 'f127', 'f137', 'f204', 'f312', 'f365', 'f96', 'f224', 'f324', 'offer_position_in_session', 'f142', 'f340', 'f97', 'f149', 'f319', 'f67', 'f124', 'f126', 'f138', 'f140', 'f361', 'f28', 'f94', 'f173', 'f186', 'f214', 'f321', 'f346', 'interest_strength_ratio', 'f90', 'f99', 'f121', 'f130', 'f134', 'f141', 'f150', 'f342', 'f22', 'f31', 'f43', 'f59', 'f107', 'f113', 'f336', 'f351', 'f74', 'f76', 'f147', 'f167', 'f195', 'f215', 'f345', 'f2', 'f9', 'f41', 'f133', 'f216', 'f313', 'hour_sin', 'f30', 'f77', 'f98', 'f101', 'f143', 'f179', 'f196', 'f209', 'f318', 'f320', 'f1', 'f5', 'f27', 'f51', 'f58', 'f60', 'f68', 'f73', 'f106', 'f108', 'f148', 'f163', 'f170', 'f172', 'f219', 'f225', 'f311', 'f317', 'f323', 'f329', 'f344', 'merchant_cat_rarity', 'f8', 'f85', 'f91', 'f105', 'f131', 'f182', 'f213', 'f261', 'interest_business_score', 'interest_total_score', 'top_interest_value', 'f46', 'f69', 'f78', 'f103', 'f146', 'f168', 'f174', 'f180', 'f198', 'f327', 'f12', 'f40', 'f55', 'f65', 'f83', 'f86', 'f111', 'f152', 'f197', 'f200', 'f341', 'f353', 'avg_key_interest', 'f4', 'f10', 'f45', 'f47', 'f49', 'f79', 'f93', 'f115', 'f116', 'f123', 'f156', 'f178', 'f181', 'f184', 'f192', 'f208', 'f217', 'f218', 'f227', 'f322', 'f338', 'f356', 'f364', 'interest_gap_f2', 'interest_gap_f8', 'interest_gap_f9', 'interest_gap_f12', 'f3', 'f6', 'f7', 'f11', 'f35', 'f50', 'f87', 'f89', 'f100', 'f109', 'f117', 'f166', 'f169', 'f183', 'f202', 'f222', 'f325', 'f328', 'f339', 'f357', 'merchant_cat_ComputerAndComputerSoftwareStores', 'hour_cos', 'interest_dining_score', 'interest_hobby_score', 'miles_utilization_ratio', 'f29', 'f42', 'f61', 'f72', 'f81', 'f82', 'f114', 'f118', 'f155', 'f158', 'f164', 'f171', 'f175', 'f177', 'f187', 'f191', 'f199', 'f211', 'f241', 'f288', 'f316', 'f326', 'f347', 'f362', 'merchant_cat_DirectSellingEstablishments', 'merchant_cat_FamilyClothingStores', 'merchant_cat_Other', 'merchant_cat_frequency', 'interest_gap_f1', 'discount_interest_ratio', 'discount_z_score', 'f32', 'f36', 'f104', 'f119', 'f157', 'f159', 'f160', 'f161', 'f162', 'f190', 'f193', 'f220', 'f231', 'f237', 'f252', 'f254', 'f269', 'f285', 'f315', 'f331', 'f355', 'f375', 'f376', 'merchant_cat_MiscellaneousFoodStores', 'interest_gap_f3', 'interest_gap_f4', 'interest_gap_f5', 'interest_gap_f7', 'interest_gap_f10', 'interest_gap_f11', 'loyalty_engagement_score', 'offers_in_same_category', 'offer_uniqueness', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f23', 'f24', 'f25', 'f33', 'f34', 'f37', 'f44', 'f48', 'f52', 'f53', 'f54', 'f56', 'f57', 'f62', 'f63', 'f64', 'f66', 'f70', 'f71', 'f75', 'f80', 'f84', 'f88', 'f92', 'f102', 'f110', 'f112', 'f120', 'f122', 'f128', 'f129', 'f135', 'f136', 'f144', 'f145', 'f153', 'f154', 'f165', 'f176', 'f185', 'f188', 'f189', 'f194', 'f201', 'f205', 'f221', 'f226', 'f228', 'f229', 'f230', 'f232', 'f233', 'f234', 'f235', 'f236', 'f238', 'f239', 'f240', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f253', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f286', 'f287', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299', 'f300', 'f301', 'f302', 'f303', 'f304', 'f305', 'f306', 'f307', 'f308', 'f309', 'f310', 'f330', 'f332', 'f333', 'f334', 'y', 'impression_time', 'click_time', 'offer_ctr_bucket', 'click_rate_30d', 'engagement_score', 'log_ctr_f137', 'exp_ctr_f134', 'category_ctr_bin', 'minute']\n","output_type":"stream"}],"execution_count":10},{"id":"d2c3e0d8-4694-4c29-a04d-216005dae43b","cell_type":"code","source":"train_features_final['id4']=pd.to_datetime(train_features_final['id4'],errors='coerce')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:27:10.815747Z","iopub.execute_input":"2025-07-26T17:27:10.816011Z","iopub.status.idle":"2025-07-26T17:27:11.024698Z","shell.execute_reply.started":"2025-07-26T17:27:10.815991Z","shell.execute_reply":"2025-07-26T17:27:11.023929Z"}},"outputs":[],"execution_count":11},{"id":"50236f42-59b9-4679-9489-c9072d6b8f43","cell_type":"code","source":"test_features_final['id4']=pd.to_datetime(test_features_final['id4'],errors='coerce')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:27:11.025461Z","iopub.execute_input":"2025-07-26T17:27:11.025673Z","iopub.status.idle":"2025-07-26T17:27:11.130450Z","shell.execute_reply.started":"2025-07-26T17:27:11.025646Z","shell.execute_reply":"2025-07-26T17:27:11.129781Z"}},"outputs":[],"execution_count":12},{"id":"ea7d9ea8-5ad8-483c-abbf-45ee6d55fcf0","cell_type":"code","source":"train_features_final=train_features_final.sort_values(by='id4',ascending=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:27:11.131198Z","iopub.execute_input":"2025-07-26T17:27:11.131475Z","iopub.status.idle":"2025-07-26T17:27:12.987484Z","shell.execute_reply.started":"2025-07-26T17:27:11.131448Z","shell.execute_reply":"2025-07-26T17:27:12.985992Z"}},"outputs":[],"execution_count":13},{"id":"0f30701f","cell_type":"code","source":"train_features_final=train_features_final.drop(columns='id5')\ntest_features_final=test_features_final.drop(columns='id5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:27:12.988140Z","iopub.execute_input":"2025-07-26T17:27:12.988797Z","iopub.status.idle":"2025-07-26T17:27:14.207627Z","shell.execute_reply.started":"2025-07-26T17:27:12.988768Z","shell.execute_reply":"2025-07-26T17:27:14.206809Z"}},"outputs":[],"execution_count":14},{"id":"c7c4cb07","cell_type":"code","source":"cols_to_convert = ['id4', 'id12', 'id13', 'impression_time', 'click_time'] # Added 'click_time' for completeness\n\n# List of DataFrames to apply the conversion to\ndataframes = [train_features_final, test_features_final]\n\nfor df in dataframes:\n    for col in cols_to_convert:\n        # Check if the column exists in the dataframe before converting\n        if col in df.columns:\n            # Convert column to datetime objects, then to Unix timestamp in seconds\n            df[col] = pd.to_datetime(df[col], errors='coerce').astype('int64') // 10**9 # Added errors='coerce' for robustness\n\n# Convert seconds to days (24 hours * 60 minutes * 60 seconds)\ntrain_features_final['days_since_offer_start'] = (train_features_final['id4'] - train_features_final['id12']) / (24 * 3600)\ntest_features_final['days_since_offer_start'] = (test_features_final['id4'] - test_features_final['id12']) / (24 * 3600)\n# Handle negative values if event timestamp is before offer start (impute to 0 or mean)\ntrain_features_final.loc[train_features_final['days_since_offer_start'] < 0, 'days_since_offer_start'] = 0\ntest_features_final.loc[test_features_final['days_since_offer_start'] < 0, 'days_since_offer_start'] = 0\n\n\n# 3. offer_duration\n# Convert seconds to days\ntrain_features_final['offer_duration'] = (train_features_final['id13'] - train_features_final['id12']) / (24 * 3600)\ntest_features_final['offer_duration'] = (test_features_final['id13'] - test_features_final['id12']) / (24 * 3600)\n# Handle negative duration if end is before start (impute to 0 or mean)\ntrain_features_final.loc[train_features_final['offer_duration'] < 0, 'offer_duration'] = 0\ntest_features_final.loc[test_features_final['offer_duration'] < 0, 'offer_duration'] = 0\n\n\n# --- Insert the Mean/Target Encoding for id3 (and id2 if desired) here ---\n# 2. id3_mean_encoded_ctr (Offer ID Mean/Target Encoding)\nfrom sklearn.model_selection import KFold\n\nNFOLDS = 5\nkf = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\ntrain_features_final['id3_mean_encoded_ctr'] = np.nan\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_features_final)):\n    fold_mean_ctr = train_features_final.iloc[train_idx].groupby('id3')['y'].mean()\n    train_features_final.loc[val_idx, 'id3_mean_encoded_ctr'] = train_features_final.iloc[val_idx]['id3'].map(fold_mean_ctr)\n\nglobal_id3_mean_ctr = train_features_final.groupby('id3')['y'].mean()\ntest_features_final['id3_mean_encoded_ctr'] = test_features_final['id3'].map(global_id3_mean_ctr)\ntest_features_final['id3_mean_encoded_ctr'] = test_features_final['id3_mean_encoded_ctr'].fillna(train_features_final['y'].mean())\n\n# (Optional) If you want to also mean-encode 'id2' (Customer ID):\n# train_features_final['id2_mean_encoded_ctr'] = np.nan\n# for fold, (train_idx, val_idx) in enumerate(kf.split(train_features_final)):\n#     fold_mean_ctr = train_features_final.iloc[train_idx].groupby('id2')['y'].mean()\n#     train_features_final.loc[val_idx, 'id2_mean_encoded_ctr'] = train_features_final.iloc[val_idx]['id2'].map(fold_mean_ctr)\n# global_id2_mean_ctr = train_features_final.groupby('id2')['y'].mean()\n# test_features_final['id2_mean_encoded_ctr'] = test_features_final['id2'].map(global_id2_mean_ctr)\n# test_features_final['id2_mean_encoded_ctr'] = test_features_final['id2_mean_encoded_ctr'].fillna(train_features_final['y'].mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:27:14.208544Z","iopub.execute_input":"2025-07-26T17:27:14.208872Z","iopub.status.idle":"2025-07-26T17:27:21.046537Z","shell.execute_reply.started":"2025-07-26T17:27:14.208846Z","shell.execute_reply":"2025-07-26T17:27:21.045750Z"}},"outputs":[],"execution_count":15},{"id":"5d6d340d","cell_type":"code","source":"X=train_features_final.drop(columns=['y','id1'])\ny=train_features_final['y']\nX_train=X[:616131]\ny_train=y[:616131]\nX_test=X[616131:]\ny_test=y[616131:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:27:21.047441Z","iopub.execute_input":"2025-07-26T17:27:21.047714Z","iopub.status.idle":"2025-07-26T17:27:21.793074Z","shell.execute_reply.started":"2025-07-26T17:27:21.047695Z","shell.execute_reply":"2025-07-26T17:27:21.792406Z"}},"outputs":[],"execution_count":16},{"id":"4e889289-474c-4c7e-bb1d-97bd4c9eec47","cell_type":"code","source":"X_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:27:21.793845Z","iopub.execute_input":"2025-07-26T17:27:21.794097Z","iopub.status.idle":"2025-07-26T17:27:21.976135Z","shell.execute_reply.started":"2025-07-26T17:27:21.794072Z","shell.execute_reply":"2025-07-26T17:27:21.975226Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"            id2     id3         id4  id10         id8        id12        id13  \\\n86947   1111176   58396  1698796823   1.0  57130000.0  1694995200  1698796799   \n86948   1111176   74465  1698796823   1.0  58120500.0  1690156800  1698796799   \n86949   1111176  566484  1698796823   2.0  59991100.0  1690848000  1698883199   \n86950   1111176  762903  1698796823   1.0  80990000.0  1690848000  1698883199   \n86951   1111176   14996  1698796823   1.0  56510000.0  1693526400  1698796799   \n...         ...     ...         ...   ...         ...         ...         ...   \n760465  1899411  689367  1699000256   1.0  53110100.0  1698796800  1704067199   \n626139  1742558  146933  1699000256   1.0  53110000.0  1693872000  1704067199   \n252976  1315150   24840  1699000256   1.0  56510500.0  1695168000  1703116799   \n252977  1315150    2552  1699000256   1.0  56510000.0  1695168000  1703116799   \n252978  1315150   86160  1699000256   1.0  56510500.0  1696377600  1703548799   \n\n        id6  id7  time_since_last_impression  ...  offer_ctr_bucket  \\\n86947   NaN  NaN                    0.000000  ...                 0   \n86948   NaN  NaN                    0.032000  ...                 0   \n86949   NaN  NaN                    0.071000  ...                 1   \n86950   NaN  NaN                    0.009000  ...                 1   \n86951   NaN  NaN                    0.004000  ...                 0   \n...     ...  ...                         ...  ...               ...   \n760465  NaN  NaN                    1.000924  ...                 3   \n626139  NaN  NaN                    0.000000  ...                 1   \n252976  NaN  NaN                    0.695000  ...                 1   \n252977  NaN  NaN                    0.001000  ...                 1   \n252978  NaN  NaN                    0.022000  ...                 1   \n\n        click_rate_30d  engagement_score  log_ctr_f137  exp_ctr_f134  \\\n86947         0.000000          0.000000      0.123859      0.865915   \n86948         0.000000          0.000000      0.123859      0.865915   \n86949         0.000000          0.000000      0.123859      0.865915   \n86950         0.000000          0.000000      0.123859      0.865915   \n86951         0.000000          0.000000      0.123859      0.865915   \n...                ...               ...           ...           ...   \n760465        0.000000          0.000000      0.101904      0.884635   \n626139        0.000000          0.000000      0.002841      1.000000   \n252976        0.142857          0.137371      0.128719      0.886920   \n252977        0.333332          0.137371      0.128719      0.886920   \n252978        0.499998          0.137371      0.128719      0.886920   \n\n        category_ctr_bin  minute  days_since_offer_start  offer_duration  \\\n86947                  5       0               44.000266       43.999988   \n86948                  5       0              100.000266       99.999988   \n86949                  5       0               92.000266       92.999988   \n86950                  5       0               92.000266       92.999988   \n86951                  5       0               61.000266       60.999988   \n...                  ...     ...                     ...             ...   \n760465                 5       0                2.354815       60.999988   \n626139                 0       0               59.354815      117.999988   \n252976                 5       0               44.354815       91.999988   \n252977                 5       0               44.354815       91.999988   \n252978                 5       0               30.354815       82.999988   \n\n        id3_mean_encoded_ctr  \n86947                    NaN  \n86948                    NaN  \n86949               0.013029  \n86950                    NaN  \n86951                    NaN  \n...                      ...  \n760465              0.139013  \n626139                   NaN  \n252976                   NaN  \n252977                   NaN  \n252978                   NaN  \n\n[616131 rows x 421 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id2</th>\n      <th>id3</th>\n      <th>id4</th>\n      <th>id10</th>\n      <th>id8</th>\n      <th>id12</th>\n      <th>id13</th>\n      <th>id6</th>\n      <th>id7</th>\n      <th>time_since_last_impression</th>\n      <th>...</th>\n      <th>offer_ctr_bucket</th>\n      <th>click_rate_30d</th>\n      <th>engagement_score</th>\n      <th>log_ctr_f137</th>\n      <th>exp_ctr_f134</th>\n      <th>category_ctr_bin</th>\n      <th>minute</th>\n      <th>days_since_offer_start</th>\n      <th>offer_duration</th>\n      <th>id3_mean_encoded_ctr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>86947</th>\n      <td>1111176</td>\n      <td>58396</td>\n      <td>1698796823</td>\n      <td>1.0</td>\n      <td>57130000.0</td>\n      <td>1694995200</td>\n      <td>1698796799</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.123859</td>\n      <td>0.865915</td>\n      <td>5</td>\n      <td>0</td>\n      <td>44.000266</td>\n      <td>43.999988</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>86948</th>\n      <td>1111176</td>\n      <td>74465</td>\n      <td>1698796823</td>\n      <td>1.0</td>\n      <td>58120500.0</td>\n      <td>1690156800</td>\n      <td>1698796799</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.032000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.123859</td>\n      <td>0.865915</td>\n      <td>5</td>\n      <td>0</td>\n      <td>100.000266</td>\n      <td>99.999988</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>86949</th>\n      <td>1111176</td>\n      <td>566484</td>\n      <td>1698796823</td>\n      <td>2.0</td>\n      <td>59991100.0</td>\n      <td>1690848000</td>\n      <td>1698883199</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.071000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.123859</td>\n      <td>0.865915</td>\n      <td>5</td>\n      <td>0</td>\n      <td>92.000266</td>\n      <td>92.999988</td>\n      <td>0.013029</td>\n    </tr>\n    <tr>\n      <th>86950</th>\n      <td>1111176</td>\n      <td>762903</td>\n      <td>1698796823</td>\n      <td>1.0</td>\n      <td>80990000.0</td>\n      <td>1690848000</td>\n      <td>1698883199</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.009000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.123859</td>\n      <td>0.865915</td>\n      <td>5</td>\n      <td>0</td>\n      <td>92.000266</td>\n      <td>92.999988</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>86951</th>\n      <td>1111176</td>\n      <td>14996</td>\n      <td>1698796823</td>\n      <td>1.0</td>\n      <td>56510000.0</td>\n      <td>1693526400</td>\n      <td>1698796799</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.004000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.123859</td>\n      <td>0.865915</td>\n      <td>5</td>\n      <td>0</td>\n      <td>61.000266</td>\n      <td>60.999988</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>760465</th>\n      <td>1899411</td>\n      <td>689367</td>\n      <td>1699000256</td>\n      <td>1.0</td>\n      <td>53110100.0</td>\n      <td>1698796800</td>\n      <td>1704067199</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000924</td>\n      <td>...</td>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.101904</td>\n      <td>0.884635</td>\n      <td>5</td>\n      <td>0</td>\n      <td>2.354815</td>\n      <td>60.999988</td>\n      <td>0.139013</td>\n    </tr>\n    <tr>\n      <th>626139</th>\n      <td>1742558</td>\n      <td>146933</td>\n      <td>1699000256</td>\n      <td>1.0</td>\n      <td>53110000.0</td>\n      <td>1693872000</td>\n      <td>1704067199</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.002841</td>\n      <td>1.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>59.354815</td>\n      <td>117.999988</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>252976</th>\n      <td>1315150</td>\n      <td>24840</td>\n      <td>1699000256</td>\n      <td>1.0</td>\n      <td>56510500.0</td>\n      <td>1695168000</td>\n      <td>1703116799</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.695000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.142857</td>\n      <td>0.137371</td>\n      <td>0.128719</td>\n      <td>0.886920</td>\n      <td>5</td>\n      <td>0</td>\n      <td>44.354815</td>\n      <td>91.999988</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>252977</th>\n      <td>1315150</td>\n      <td>2552</td>\n      <td>1699000256</td>\n      <td>1.0</td>\n      <td>56510000.0</td>\n      <td>1695168000</td>\n      <td>1703116799</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.001000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.333332</td>\n      <td>0.137371</td>\n      <td>0.128719</td>\n      <td>0.886920</td>\n      <td>5</td>\n      <td>0</td>\n      <td>44.354815</td>\n      <td>91.999988</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>252978</th>\n      <td>1315150</td>\n      <td>86160</td>\n      <td>1699000256</td>\n      <td>1.0</td>\n      <td>56510500.0</td>\n      <td>1696377600</td>\n      <td>1703548799</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.022000</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.499998</td>\n      <td>0.137371</td>\n      <td>0.128719</td>\n      <td>0.886920</td>\n      <td>5</td>\n      <td>0</td>\n      <td>30.354815</td>\n      <td>82.999988</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>616131 rows × 421 columns</p>\n</div>"},"metadata":{}}],"execution_count":17},{"id":"caffb51d","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import train_test_split\n\n# Compute scale_pos_weight for class imbalance\nscale_pos_weight = (y == 0).sum() / (y == 1).sum()\nprint(f\"scale_pos_weight = {scale_pos_weight:.2f}\")\n\n# Best params you recovered\nparams = {\n    \"learning_rate\": 0.04588242171630075,\n    \"num_leaves\": 39,\n    \"max_depth\": 10,\n    \"min_child_samples\": 25,\n    \"feature_fraction\": 0.5269864948058149,\n    \"bagging_fraction\": 0.9954022568460998,\n    \"bagging_freq\": 2,\n    \"lambda_l1\": 0.015544600650003099,\n    \"lambda_l2\": 2.1832264146154266e-08,\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",  # Can keep this even if optimizing for PR-AUC externally\n    \"verbosity\": -1,\n    \"boosting_type\": \"gbdt\",\n    \"device\": \"gpu\",  # Remove this if no GPU is available\n    \"scale_pos_weight\": 1,\n}\n\n# Train on full dataset (or use train/test split to evaluate if needed)\nfinal_train = lgb.Dataset(X_train, y_train)\nmodel = lgb.train(params, final_train, num_boost_round=1000)\n\n# Optionally evaluate on test split if X_test/y_test available\nif 'X_test' in locals():\n    preds = model.predict(X_test)\n    pr_auc = average_precision_score(y_test, preds)\n    print(f\"PR-AUC on holdout set: {pr_auc:.4f}\")\n\n# Save model\nmodel.save_model(\"lgbm_classweighted_optuna_pr_auc.txt\")\nprint(\"✅ Model retrained and saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T17:27:21.977151Z","iopub.execute_input":"2025-07-26T17:27:21.977519Z","iopub.status.idle":"2025-07-26T17:29:27.704732Z","shell.execute_reply.started":"2025-07-26T17:27:21.977499Z","shell.execute_reply":"2025-07-26T17:29:27.703636Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"scale_pos_weight = 19.79\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"PR-AUC on holdout set: 0.7469\n✅ Model retrained and saved.\n","output_type":"stream"}],"execution_count":18},{"id":"4ba14b7b-c194-402a-be7b-50086655a1aa","cell_type":"code","source":"feat_imp = pd.DataFrame({\n    'feature': model.feature_name(),\n    'importance': model.feature_importance(importance_type='gain')  # or 'gain'\n}).sort_values(by='importance', ascending=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T20:51:05.893407Z","iopub.execute_input":"2025-07-25T20:51:05.893991Z","iopub.status.idle":"2025-07-25T20:51:05.901676Z","shell.execute_reply.started":"2025-07-25T20:51:05.893972Z","shell.execute_reply":"2025-07-25T20:51:05.901054Z"}},"outputs":[],"execution_count":20},{"id":"f93513f5-4b6d-4ba4-a027-8ea2cf9fd038","cell_type":"code","source":"feat_imp_df = pd.DataFrame({\n    'Variable': model.feature_name(),\n    'Importance_Score': model.feature_importance(importance_type='gain')\n}).sort_values(by='Importance_Score', ascending=False).reset_index(drop=True)\n\n# You can add a 'Rank' column if desired\nfeat_imp_df['Rank'] = feat_imp_df.index + 1\n\n# Reorder columns for better presentation\nfeat_imp_df = feat_imp_df[['Rank', 'Variable', 'Importance_Score']]\n\n# Define the output file name\noutput_csv_file = 'variable_importance_for_submission.csv'\n\n# Save the DataFrame to a CSV file\nfeat_imp_df.to_csv(output_csv_file, index=False)\n\nprint(f\"\\n✅ Variable importance file '{output_csv_file}' created successfully!\")\nprint(f\"It contains {len(feat_imp_df)} variables.\")\nprint(\"\\nFirst 30 rows of the importance file:\")\nprint(feat_imp_df.head(30))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T09:42:15.900247Z","iopub.execute_input":"2025-07-26T09:42:15.900479Z","iopub.status.idle":"2025-07-26T09:42:15.917864Z","shell.execute_reply.started":"2025-07-26T09:42:15.900463Z","shell.execute_reply":"2025-07-26T09:42:15.917186Z"}},"outputs":[{"name":"stdout","text":"\n✅ Variable importance file 'variable_importance_for_submission.csv' created successfully!\nIt contains 421 variables.\n\nFirst 30 rows of the importance file:\n    Rank                     Variable  Importance_Score\n0      1   time_since_last_impression     716198.770964\n1      2                         f366     402674.544489\n2      3                         f363     143538.844168\n3      4                         f137      96671.873459\n4      5             offer_ctr_bucket      66029.619229\n5      6                 log_ctr_f137      54913.177975\n6      7                         f210      54605.794117\n7      8                         f125      47033.955250\n8      9                         f132      34413.605288\n9     10          user_session_length      29160.861381\n10    11                         id12      26123.675611\n11    12                         f353      20166.022076\n12    13                         f207      15402.721170\n13    14                         f365      14402.594923\n14    15      user_avg_session_length      13990.158935\n15    16                         f127      13638.864761\n16    17       hourly_offer_frequency      12915.556296\n17    18                 exp_ctr_f134      10969.781196\n18    19  user_cumulative_impressions      10771.465449\n19    20                         f223      10633.039634\n20    21                         f313      10114.483541\n21    22    offer_position_in_session       8364.474712\n22    23               click_rate_30d       8198.800566\n23    24                         f311       7856.533650\n24    25                         f358       6891.420911\n25    26                         f337       6381.390704\n26    27                         f139       6309.704602\n27    28       days_since_offer_start       6305.145092\n28    29                         f126       6299.140384\n29    30                         f314       6224.481079\n","output_type":"stream"}],"execution_count":20},{"id":"9d05bc80-e7e4-4459-b9fb-cd752532f1d9","cell_type":"code","source":"top300=feat_imp['feature'][:320].tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T20:51:05.902324Z","iopub.execute_input":"2025-07-25T20:51:05.902592Z","iopub.status.idle":"2025-07-25T20:51:05.919995Z","shell.execute_reply.started":"2025-07-25T20:51:05.902570Z","shell.execute_reply":"2025-07-25T20:51:05.919594Z"}},"outputs":[],"execution_count":21},{"id":"1767f2d4-fae7-4a2b-947e-0d371da248a5","cell_type":"code","source":"top300.append('id1')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T20:51:05.920630Z","iopub.execute_input":"2025-07-25T20:51:05.920830Z","iopub.status.idle":"2025-07-25T20:51:05.938796Z","shell.execute_reply.started":"2025-07-25T20:51:05.920814Z","shell.execute_reply":"2025-07-25T20:51:05.938278Z"}},"outputs":[],"execution_count":22},{"id":"b863c205-cf8f-42ff-8af2-43f800e7f9f3","cell_type":"code","source":"print(\"\\n--- New Feat Imp (Top Pachaas) ---\")\nprint(feat_imp.head(50))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T20:51:05.939617Z","iopub.execute_input":"2025-07-25T20:51:05.939840Z","iopub.status.idle":"2025-07-25T20:51:05.959243Z","shell.execute_reply.started":"2025-07-25T20:51:05.939826Z","shell.execute_reply":"2025-07-25T20:51:05.958705Z"}},"outputs":[{"name":"stdout","text":"\n--- New Feat Imp (Top Pachaas) ---\n                         feature     importance\n9     time_since_last_impression  716477.763930\n11                          f366  402809.727688\n18                          f363  143524.676861\n36                          f137   99707.214157\n411             offer_ctr_bucket   66037.856720\n19                          f210   54618.039416\n414                 log_ctr_f137   51904.371281\n15                          f125   47164.186733\n14                          f132   34463.202847\n10           user_session_length   29288.687277\n5                           id12   26041.226748\n157                         f353   20193.652378\n16                          f207   15397.296996\n39                          f365   14305.923556\n23       user_avg_session_length   13993.790203\n35                          f127   13657.290727\n12        hourly_offer_frequency   13039.739840\n415                 exp_ctr_f134   11009.379808\n17                          f223   10649.192960\n13   user_cumulative_impressions   10581.429802\n91                          f313    9952.355563\n43     offer_position_in_session    8533.373561\n412               click_rate_30d    8156.757707\n119                         f311    7742.894497\n22                          f358    7054.671435\n25                          f337    6387.679020\n51                          f126    6331.920872\n27                          f139    6279.682850\n418       days_since_offer_start    6227.040218\n20                          f314    6213.163950\n88                           f41    5860.252882\n413             engagement_score    5857.556453\n93                           f30    5130.514750\n21                          f206    4924.674180\n52                          f138    4892.336022\n68                          f141    4872.208108\n38                          f312    4821.839993\n33                          f350    4711.932283\n31                          f203    4607.236028\n94                           f77    4555.513433\n0                            id2    4456.659206\n109                          f68    4428.834533\n95                           f98    4219.017016\n61                          f346    4218.885627\n75                          f107    4200.318884\n181                         f364    4116.224816\n85                          f345    4110.256982\n30                          f212    4084.342828\n100                         f209    3921.236436\n71                           f22    3900.413104\n","output_type":"stream"}],"execution_count":23},{"id":"ae21e238-ff36-4e16-b769-55cde69ae779","cell_type":"code","source":"# train_features_final.to_csv('finaltrainig.csv')\n# test_features_final.to_csv('finaltestig.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.470185Z","iopub.status.idle":"2025-07-25T18:15:05.470573Z","shell.execute_reply.started":"2025-07-25T18:15:05.470389Z","shell.execute_reply":"2025-07-25T18:15:05.470404Z"}},"outputs":[],"execution_count":null},{"id":"a8952e5a-d7e7-4310-9c55-b5095afa7493","cell_type":"code","source":"test_filtered=test_features_final[top300]\ntop300.append('y')\ntrain_filtered=train_features_final[top300]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.471900Z","iopub.status.idle":"2025-07-25T18:15:05.472241Z","shell.execute_reply.started":"2025-07-25T18:15:05.472054Z","shell.execute_reply":"2025-07-25T18:15:05.472068Z"}},"outputs":[],"execution_count":null},{"id":"80db0459-fc01-4eb5-b883-27001f1e7636","cell_type":"code","source":"print(test_filtered.shape,train_filtered.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.472968Z","iopub.status.idle":"2025-07-25T18:15:05.473313Z","shell.execute_reply.started":"2025-07-25T18:15:05.473124Z","shell.execute_reply":"2025-07-25T18:15:05.473137Z"}},"outputs":[],"execution_count":null},{"id":"a7edced4-e054-4b26-93c3-67b7ff213361","cell_type":"code","source":"X=train_filtered.drop(columns=['y','id1'])\ny=train_filtered['y']\nX_train=X[:616131]\ny_train=y[:616131]\nX_test=X[616131:]\ny_test=y[616131:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.474067Z","iopub.status.idle":"2025-07-25T18:15:05.474398Z","shell.execute_reply.started":"2025-07-25T18:15:05.474240Z","shell.execute_reply":"2025-07-25T18:15:05.474255Z"}},"outputs":[],"execution_count":null},{"id":"b86413be-d9de-4d39-8ace-b9b97cdd3a8f","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import train_test_split\n\n\n# Compute scale_pos_weight for class imbalance\nscale_pos_weight = (y == 0).sum() / (y == 1).sum()\nprint(f\"scale_pos_weight = {scale_pos_weight:.2f}\")\n\n# Best params you recovered\nparams = {\n    'learning_rate': 0.016623218477757856, 'num_leaves': 88, 'max_depth': 12, 'min_child_samples': 31, 'feature_fraction': 0.7857024999905345, 'bagging_fraction': 0.8106542792647912, 'bagging_freq': 2, 'lambda_l1': 9.625481788587463, 'lambda_l2': 4.969459532508245e-05,\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",  # Can keep this even if optimizing for PR-AUC externally\n    \"verbosity\": -1,\n    \"boosting_type\": \"gbdt\",\n    \"device\": \"gpu\",  # Remove this if no GPU is available\n    \"scale_pos_weight\": 1,\n}\n\n# Train on full dataset (or use train/test split to evaluate if needed)\nfinal_train = lgb.Dataset(X, y)\nmodel_final = lgb.train(params, final_train, num_boost_round=1000)\n\n# Optionally evaluate on test split if X_test/y_test available\nif 'X_test' in locals():\n    preds = model_final.predict(X_test)\n    pr_auc = average_precision_score(y_test, preds)\n    print(f\"PR-AUC on holdout set: {pr_auc:.4f}\")\n\n# Save model\nmodel_final.save_model(\"lgbm_classweighted_optuna_pr_auc1.txt\")\nprint(\"✅ Model retrained and saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.475653Z","iopub.status.idle":"2025-07-25T18:15:05.475943Z","shell.execute_reply.started":"2025-07-25T18:15:05.475778Z","shell.execute_reply":"2025-07-25T18:15:05.475793Z"},"scrolled":true},"outputs":[],"execution_count":null},{"id":"de807b58-f35d-4365-a355-89259637a504","cell_type":"code","source":"df_test=pd.read_csv('/kaggle/input/kanafrendatasetsplswork/test_features_final.csv')\ndf_test=df_test[['id1','id2','id3','id5']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.476821Z","iopub.status.idle":"2025-07-25T18:15:05.477149Z","shell.execute_reply.started":"2025-07-25T18:15:05.476966Z","shell.execute_reply":"2025-07-25T18:15:05.476980Z"}},"outputs":[],"execution_count":null},{"id":"5be4f28c-6c6f-4b6e-b89e-3231559af657","cell_type":"code","source":"test_filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.478434Z","iopub.status.idle":"2025-07-25T18:15:05.478652Z","shell.execute_reply.started":"2025-07-25T18:15:05.478543Z","shell.execute_reply":"2025-07-25T18:15:05.478551Z"}},"outputs":[],"execution_count":null},{"id":"bcb17c6a-0d2c-487f-be1d-a95e118505eb","cell_type":"code","source":"pred_test_filtered=test_filtered.drop(columns='id1')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.479700Z","iopub.status.idle":"2025-07-25T18:15:05.479969Z","shell.execute_reply.started":"2025-07-25T18:15:05.479852Z","shell.execute_reply":"2025-07-25T18:15:05.479866Z"}},"outputs":[],"execution_count":null},{"id":"31f7a76b-4684-4bf1-9d03-56a62f5a7fe3","cell_type":"code","source":"pred_test_filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.481290Z","iopub.status.idle":"2025-07-25T18:15:05.481500Z","shell.execute_reply.started":"2025-07-25T18:15:05.481400Z","shell.execute_reply":"2025-07-25T18:15:05.481409Z"}},"outputs":[],"execution_count":null},{"id":"56868101-5752-441d-a4cf-0138c6cd3476","cell_type":"code","source":"prediction=model_final.predict(pred_test_filtered)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.483009Z","iopub.status.idle":"2025-07-25T18:15:05.483351Z","shell.execute_reply.started":"2025-07-25T18:15:05.483173Z","shell.execute_reply":"2025-07-25T18:15:05.483187Z"}},"outputs":[],"execution_count":null},{"id":"3552fc97-e1ec-43c4-8ee9-a8733e6c91f5","cell_type":"code","source":"prediction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.484692Z","iopub.status.idle":"2025-07-25T18:15:05.485320Z","shell.execute_reply.started":"2025-07-25T18:15:05.485153Z","shell.execute_reply":"2025-07-25T18:15:05.485166Z"}},"outputs":[],"execution_count":null},{"id":"9e6e9f12-4bbf-4af1-82fe-c3356721dca7","cell_type":"code","source":"test_filtered['pred']=prediction\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.486310Z","iopub.status.idle":"2025-07-25T18:15:05.486596Z","shell.execute_reply.started":"2025-07-25T18:15:05.486430Z","shell.execute_reply":"2025-07-25T18:15:05.486442Z"}},"outputs":[],"execution_count":null},{"id":"0dfedf1d-3714-4b34-aefe-d2e904b18282","cell_type":"code","source":"test_filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.487415Z","iopub.status.idle":"2025-07-25T18:15:05.487617Z","shell.execute_reply.started":"2025-07-25T18:15:05.487521Z","shell.execute_reply":"2025-07-25T18:15:05.487529Z"}},"outputs":[],"execution_count":null},{"id":"025f7518-5185-4f93-88d8-ac88bb619224","cell_type":"code","source":"submission_df=df_test.merge(test_filtered[['id1','pred']], on='id1', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.488858Z","iopub.status.idle":"2025-07-25T18:15:05.489071Z","shell.execute_reply.started":"2025-07-25T18:15:05.488967Z","shell.execute_reply":"2025-07-25T18:15:05.488975Z"}},"outputs":[],"execution_count":null},{"id":"c753a65d-7e22-4612-9f98-67ff8c144e77","cell_type":"code","source":"submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.490129Z","iopub.status.idle":"2025-07-25T18:15:05.490442Z","shell.execute_reply.started":"2025-07-25T18:15:05.490276Z","shell.execute_reply":"2025-07-25T18:15:05.490291Z"}},"outputs":[],"execution_count":null},{"id":"75bdb35e-0607-4772-b939-64474915ad40","cell_type":"code","source":"submission_df.to_csv('finalsubmissionplswork.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.491273Z","iopub.status.idle":"2025-07-25T18:15:05.491493Z","shell.execute_reply.started":"2025-07-25T18:15:05.491382Z","shell.execute_reply":"2025-07-25T18:15:05.491390Z"}},"outputs":[],"execution_count":null},{"id":"0ea47ab2-b5dc-4a13-b36e-accb25c35fa6","cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import lightgbm as lgb\n# from sklearn.metrics import average_precision_score\n# from sklearn.model_selection import KFold # For Mean/Target Encoding\n\n# # Load DataFrames\n# train_features_final = pd.read_csv('/kaggle/input/train_features_final.csv')\n# test_features_final = pd.read_csv('/kaggle/input/test_features_final.csv')\n# final_feature_names = pd.read_csv('/kaggle/input/final_feature_names.csv')\n\n# # --- Existing Feature Engineering from your notebook (re-ordered for logical flow) ---\n\n# # Handle id5 date conversion early for minute extraction\n# train_features_final[\"id5\"] = pd.to_datetime(train_features_final['id5'], errors='coerce')\n# test_features_final[\"id5\"] = pd.to_datetime(test_features_final['id5'], errors='coerce')\n\n# train_features_final[\"minute\"] = train_features_final['id5'].dt.minute\n# test_features_final[\"minute\"] = test_features_final['id5'].dt.minute\n\n# # Extract hour from id4 (event timestamp) and convert to sin/cos features\n# # Assuming id4 will be converted to datetime later for this purpose if it's not already\n# # If id4 is directly unix timestamp, you can convert it to datetime first here\n# # For now, let's keep id4 conversion logic later, but add hour feature based on it\n# # You had 'hour_sin' and 'hour_cos' in your feature list, let's re-create them properly\n# # based on 'id4' once it's a timestamp.\n# # For now, we'll create them after id4 is processed to Unix timestamp.\n\n\n# # Total popularity of an offer across all users\n# global_offer_stats = train_features_final.groupby('id3').agg(\n#     global_offer_impressions=('id1', 'count'),\n#     global_offer_clicks=('y', 'sum')\n# ).reset_index()\n\n# global_offer_stats['global_offer_ctr'] = global_offer_stats['global_offer_clicks'] / (global_offer_stats['global_offer_impressions'] + 1e-5) # Added epsilon for stability\n\n# bins = [0, 0.01, 0.03, 0.07, 0.15, 1]\n# labels = [0,1,2,3,4]\n# global_offer_stats['offer_ctr_bucket'] = pd.cut(global_offer_stats['global_offer_ctr'], bins=bins, labels=labels, include_lowest=True)\n\n# train_features_final = train_features_final.merge(\n#     global_offer_stats[['id3', 'offer_ctr_bucket']],\n#     on='id3',\n#     how='left'\n# )\n# test_features_final = test_features_final.merge(\n#     global_offer_stats[['id3', 'offer_ctr_bucket']],\n#     on='id3',\n#     how='left'\n# )\n# # Fill NaN for new offers in test set\n# test_features_final['offer_ctr_bucket'] = test_features_final['offer_ctr_bucket'].fillna(train_features_final['offer_ctr_bucket'].mode()[0])\n\n\n# # Step 1: Map category one-hot to corresponding CTR feature\n# category_ctr_map = {\n#     'f227': 'f130',  # Dining\n#     'f228': 'f131',  # Entertainment\n#     'f230': 'f134',  # Services\n#     'f231': 'f132',  # Shopping\n#     'f232': 'f133',  # Travel\n# }\n\n# # Step 2: Create category_ctr by using the matching category's CTR\n# def get_category_ctr(row):\n#     for cat_flag, ctr_col in category_ctr_map.items():\n#         if cat_flag in row and row[cat_flag] == 1: # Check if category flag exists and is 1\n#             return row[ctr_col]\n#     return np.nan  # fallback in case no match\n\n# train_features_final['category_ctr'] = train_features_final.apply(get_category_ctr, axis=1)\n# test_features_final['category_ctr'] = test_features_final.apply(get_category_ctr, axis=1)\n\n# # Drop NaNs for plotting clarity\n# plot_df = train_features_final[['category_ctr', 'y']].dropna().copy() # .copy() to avoid SettingWithCopyWarning\n\n# # Bin CTR for plotting clarity\n# if not plot_df.empty:\n#     plot_df['ctr_bin'] = pd.cut(plot_df['category_ctr'], bins=10)\n\n#     # Grouped mean redemption rate per CTR bin\n#     grouped = plot_df.groupby('ctr_bin')['y'].mean().reset_index()\n#     grouped['ordinal_label'] = grouped['y'].rank(method='first').astype(int) - 1\n\n#     grouped['interval'] = pd.IntervalIndex.from_tuples([\n#         (-0.001, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 0.4), (0.4, 0.5),\n#         (0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)\n#     ])\n#     bin_to_label = dict(zip(grouped['interval'], grouped['ordinal_label']))\n#     bins = [-0.001, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n\n#     # Step 4: Map to new label based on redemption rate\n#     # Apply pd.cut directly to the full dataframes\n#     train_features_final['category_ctr_bin'] = pd.cut(train_features_final['category_ctr'], bins=bins).map(bin_to_label).astype('Int64')\n#     test_features_final['category_ctr_bin'] = pd.cut(test_features_final['category_ctr'], bins=bins).map(bin_to_label).astype('Int64')\n# else:\n#     print(\"Warning: plot_df is empty. Cannot create category_ctr_bin based on redemption rate.\")\n#     train_features_final['category_ctr_bin'] = train_features_final['category_ctr'].fillna(train_features_final['category_ctr'].median()).astype('Int64') # Fallback if empty\n#     test_features_final['category_ctr_bin'] = test_features_final['category_ctr'].fillna(test_features_final['category_ctr'].median()).astype('Int64') # Fallback if empty\n\n\n# train_features_final = train_features_final.drop(columns='category_ctr')\n# test_features_final = test_features_final.drop(columns='category_ctr')\n\n# # Convert relevant columns to datetime and then Unix timestamp in seconds\n# cols_to_convert_to_unix = ['id4', 'id12', 'id13', 'impression_time', 'click_time'] # Added 'click_time' for completeness, though often target\n# # Ensure 'id1' is also kept as a string initially if it's the primary key as it's not converted to unix timestamp.\n# # It seems 'id1' contains string values like '1000061_9914_16-23_2023-11-05 09:11:35.557'\n\n# for df in [train_features_final, test_features_final]:\n#     for col in cols_to_convert_to_unix:\n#         if col in df.columns and df[col].dtype != 'int64': # Only convert if not already int64 (Unix)\n#             df[col] = pd.to_datetime(df[col], errors='coerce').astype('int64') // 10**9\n\n# # Drop id5 after minute is extracted, as per your original notebook flow\n# # If id5 is not used for anything else. If you use it for date features, keep it until then.\n# # Your original code drops it here: train_features_final=train_features_final.drop(columns='id5')\n# # Let's keep it until after new date features are made based on it, then drop.\n# # For now, let's proceed with dropping as per original notebook.\n# # train_features_final=train_features_final.drop(columns='id5')\n# # test_features_final=test_features_final.drop(columns='id5')\n\n\n# # --- Start of Jahnavi's new feature additions (incorporating previous suggestions) ---\n\n# # 1. click_rate_30d = f206 / (f207 + 1e-5)\n# train_features_final['click_rate_30d'] = train_features_final['f206'] / (train_features_final['f207'] + 1e-5)\n# test_features_final['click_rate_30d'] = test_features_final['f206'] / (test_features_final['f207'] + 1e-5)\n\n# # 2. days_since_offer_start = (timestamp - offer_start_timestamp) in days\n# # id4 is 'Event timestamp', id12 is 'Start Timestamp' (now Unix)\n# train_features_final['days_since_offer_start'] = (train_features_final['id4'] - train_features_final['id12']) / (24 * 3600)\n# test_features_final['days_since_offer_start'] = (test_features_final['id4'] - test_features_final['id12']) / (24 * 3600)\n# # Handle negative values if event timestamp is before offer start (impute to 0 or mean)\n# train_features_final.loc[train_features_final['days_since_offer_start'] < 0, 'days_since_offer_start'] = 0\n# test_features_final.loc[test_features_final['days_since_offer_start'] < 0, 'days_since_offer_start'] = 0\n\n\n# # 3. offer_duration = offer_end - offer_start in days\n# # id13 is 'End Timestamp', id12 is 'Start Timestamp' (now Unix)\n# train_features_final['offer_duration'] = (train_features_final['id13'] - train_features_final['id12']) / (24 * 3600)\n# test_features_final['offer_duration'] = (test_features_final['id13'] - test_features_final['id12']) / (24 * 3600)\n# # Handle negative duration if end is before start (impute to 0 or mean)\n# train_features_final.loc[train_features_final['offer_duration'] < 0, 'offer_duration'] = 0\n# test_features_final.loc[test_features_final['offer_duration'] < 0, 'offer_duration'] = 0\n\n\n# # 4. engagement_score = f137 × f206\n# train_features_final['engagement_score'] = train_features_final['f137'] * train_features_final['f206']\n# test_features_final['engagement_score'] = test_features_final['f137'] * test_features_final['f206']\n\n# # 5. log_ctr = log1p(f137)\n# train_features_final['log_ctr_f137'] = np.log1p(train_features_final['f137'])\n# test_features_final['log_ctr_f137'] = np.log1p(test_features_final['f137'])\n\n# # 6. exp_ctr = np.exp(-f134)\n# train_features_final['exp_ctr_f134'] = np.exp(-train_features_final['f134'])\n# test_features_final['exp_ctr_f134'] = np.exp(-test_features_final['f134'])\n\n# # --- End of Jahnavi's new feature additions ---\n\n\n# # --- Start of Additional Suggested Features ---\n\n# # 1. Time of Day/Week Interaction\n# # Extract hour from id4 (Event timestamp - now Unix)\n# # Convert Unix timestamp back to datetime temporarily for hour extraction\n# train_features_final['event_datetime'] = pd.to_datetime(train_features_final['id4'], unit='s')\n# test_features_final['event_datetime'] = pd.to_datetime(test_features_final['id4'], unit='s')\n\n# train_features_final['hour'] = train_features_final['event_datetime'].dt.hour\n# test_features_final['hour'] = test_features_final['event_datetime'].dt.hour\n\n# train_features_final['day_of_week'] = train_features_final['event_datetime'].dt.dayofweek # Monday=0, Sunday=6\n# test_features_final['day_of_week'] = test_features_final['event_datetime'].dt.dayofweek\n\n# # Cyclical features for hour (already present as 'hour_sin', 'hour_cos' in your column list)\n# train_features_final['hour_sin'] = np.sin(2 * np.pi * train_features_final['hour'] / 24)\n# train_features_final['hour_cos'] = np.cos(2 * np.pi * train_features_final['hour'] / 24)\n# test_features_final['hour_sin'] = np.sin(2 * np.pi * test_features_final['hour'] / 24)\n# test_features_final['hour_cos'] = np.cos(2 * np.pi * test_features_final['hour'] / 24)\n\n# # Interaction: hourly_offer_frequency_x_category_ctr\n# # Ensure 'hourly_offer_frequency' exists (it was in your feature list)\n# if 'hourly_offer_frequency' in train_features_final.columns:\n#     train_features_final['hourly_freq_x_cat_ctr_bin'] = train_features_final['hourly_offer_frequency'] * train_features_final['category_ctr_bin']\n#     test_features_final['hourly_freq_x_cat_ctr_bin'] = test_features_final['hourly_offer_frequency'] * test_features_final['category_ctr_bin']\n# else:\n#     print(\"Warning: 'hourly_offer_frequency' not found, skipping 'hourly_freq_x_cat_ctr_bin'\")\n#     train_features_final['hourly_freq_x_cat_ctr_bin'] = 0 # Placeholder\n#     test_features_final['hourly_freq_x_cat_ctr_bin'] = 0 # Placeholder\n\n\n# # Interaction: cm_ctr_x_offer_industry_ctr\n# # Ensure f366 and f363 exist and are numeric\n# train_features_final['cm_ctr_x_offer_industry_ctr'] = train_features_final['f366'] * train_features_final['f363']\n# test_features_final['cm_ctr_x_offer_industry_ctr'] = test_features_final['f366'] * test_features_final['f363']\n\n# # Interaction: log_ctr_f137_x_offer_ctr_bucket\n# train_features_final['log_ctr_f137_x_offer_bucket'] = train_features_final['log_ctr_f137'] * train_features_final['offer_ctr_bucket']\n# test_features_final['log_ctr_f137_x_offer_bucket'] = test_features_final['log_ctr_f137'] * test_features_final['offer_ctr_bucket']\n\n# # Ratio of Decaying to Non-Decaying Impressions/Clicks (if f148, f147, f207, f206 exist)\n# if 'f148' in train_features_final.columns and 'f207' in train_features_final.columns:\n#     train_features_final['decay_imp_30d_ratio'] = train_features_final['f148'] / (train_features_final['f207'] + 1e-5)\n#     test_features_final['decay_imp_30d_ratio'] = test_features_final['f148'] / (test_features_final['f207'] + 1e-5)\n# else:\n#     print(\"Warning: Missing f148 or f207 for 'decay_imp_30d_ratio'\")\n#     train_features_final['decay_imp_30d_ratio'] = 0\n#     test_features_final['decay_imp_30d_ratio'] = 0\n\n# if 'f147' in train_features_final.columns and 'f206' in train_features_final.columns:\n#     train_features_final['decay_clicks_30d_ratio'] = train_features_final['f147'] / (train_features_final['f206'] + 1e-5)\n#     test_features_final['decay_clicks_30d_ratio'] = test_features_final['f147'] / (test_features_final['f206'] + 1e-5)\n# else:\n#     print(\"Warning: Missing f147 or f206 for 'decay_clicks_30d_ratio'\")\n#     train_features_final['decay_clicks_30d_ratio'] = 0\n#     test_features_final['decay_clicks_30d_ratio'] = 0\n\n# # Interest Score Aggregations (example: max interest related to dining)\n# # This requires mapping offer categories to interest IDs. Let's assume a simplified mapping for demonstration\n# # You would need to refine this based on your understanding of 'f_interest_id' and 'f_offer_category' mappings\n# # For example, if offer is 'dining' (f227=1), use f8 (restaurant interest score)\n# interest_score_map = {\n#     'f227': 'f8',   # Dining -> Restaurant interest\n#     'f231': 'f41' # Shopping -> assuming f41 is related to general consumption, or pick a specific shopping interest\n#     # Add more mappings based on your actual feature mapping\n# }\n\n# def get_relevant_interest_score(row):\n#     relevant_score = 0\n#     for cat_flag, interest_col in interest_score_map.items():\n#         if cat_flag in row and row[cat_flag] == 1 and interest_col in row:\n#             # We are taking the score of the interest related to the offer's category\n#             relevant_score = max(relevant_score, row[interest_col])\n#     return relevant_score\n\n# train_features_final['relevant_interest_score'] = train_features_final.apply(get_relevant_interest_score, axis=1)\n# test_features_final['relevant_interest_score'] = test_features_final.apply(get_relevant_interest_score, axis=1)\n\n\n# # --- Mean/Target Encoding for id3 (Offer ID) and id2 (Customer ID) ---\n# # This is a powerful technique, but requires careful handling to avoid data leakage.\n# # We'll use KFold for training data and simple mean for test data.\n\n# NFOLDS = 5 # Number of folds for K-Fold target encoding\n# kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=42) # Use a fixed random state for reproducibility\n\n# # Mean encode 'id3' (Offer ID)\n# train_features_final['id3_mean_encoded_ctr'] = np.nan\n# for fold, (train_idx, val_idx) in enumerate(kf.split(train_features_final)):\n#     # Calculate mean on the training fold only\n#     fold_mean_ctr = train_features_final.iloc[train_idx].groupby('id3')['y'].mean()\n#     # Apply to the validation fold\n#     train_features_final.loc[val_idx, 'id3_mean_encoded_ctr'] = train_features_final.iloc[val_idx]['id3'].map(fold_mean_ctr)\n\n# # For test_features_final, use the mean calculated from the entire train_features_final\n# global_id3_mean_ctr = train_features_final.groupby('id3')['y'].mean()\n# test_features_final['id3_mean_encoded_ctr'] = test_features_final['id3'].map(global_id3_mean_ctr)\n# # Handle new id3s in test set (not seen in training) by filling with global mean y\n# test_features_final['id3_mean_encoded_ctr'] = test_features_final['id3_mean_encoded_ctr'].fillna(train_features_final['y'].mean())\n\n\n# # Mean encode 'id2' (Customer ID) - similar logic\n# train_features_final['id2_mean_encoded_ctr'] = np.nan\n# for fold, (train_idx, val_idx) in enumerate(kf.split(train_features_final)):\n#     fold_mean_ctr = train_features_final.iloc[train_idx].groupby('id2')['y'].mean()\n#     train_features_final.loc[val_idx, 'id2_mean_encoded_ctr'] = train_features_final.iloc[val_idx]['id2'].map(fold_mean_ctr)\n\n# global_id2_mean_ctr = train_features_final.groupby('id2')['y'].mean()\n# test_features_final['id2_mean_encoded_ctr'] = test_features_final['id2'].map(global_id2_mean_ctr)\n# test_features_final['id2_mean_encoded_ctr'] = test_features_final['id2_mean_encoded_ctr'].fillna(train_features_final['y'].mean())\n\n# # --- End of Additional Suggested Features ---\n\n# # Final timestamp dropping (assuming id5, event_datetime, hour, day_of_week are now used for features)\n# # It's generally good practice to keep original ID columns (id1, id2, id3, id4) for debugging/merging.\n# # Only drop id5 if no other datetime features will be derived from it.\n# train_features_final = train_features_final.drop(columns=['id5', 'event_datetime'])\n# test_features_final = test_features_final.drop(columns=['id5', 'event_datetime'])\n\n\n# # Sort by id4 (event timestamp) for time-based splitting\n# train_features_final = train_features_final.sort_values(by='id4', ascending=True)\n\n# # Define X and y for the model\n# # Drop 'id1' (Primary Key) and 'y' (Target) from features\n# # Also drop 'click_time' if it exists and is not intended as a feature itself but rather for 'y' derivation\n# columns_to_drop = ['y', 'id1']\n# if 'click_time' in train_features_final.columns:\n#     columns_to_drop.append('click_time')\n\n# X = train_features_final.drop(columns=columns_to_drop)\n# y = train_features_final['y']\n\n# # Filter out non-numeric columns that might have slipped through (e.g., if a feature was completely NaN)\n# # Or if 'id1' (the string-based one) accidentally got into X after drops.\n# # This ensures all features passed to LightGBM are numeric.\n# X_cols = X.select_dtypes(include=np.number).columns.tolist()\n# X = X[X_cols]\n# test_filtered_cols = test_features_final.select_dtypes(include=np.number).columns.tolist()\n# # Ensure test set has the same columns as train set before making predictions\n# # Identify columns in X that are not in test_features_final\n# missing_in_test = set(X.columns) - set(test_features_final.columns)\n# for col in missing_in_test:\n#     # Add missing columns to test_features_final with a default value (e.g., 0 or mean of train)\n#     test_features_final[col] = 0 # Or use X[col].mean()\n\n# # Align columns between training and test sets after feature engineering\n# test_filtered = test_features_final[X.columns]\n\n\n# # Split into training and validation sets based on chronological order\n# # Use the same split point as your original notebook for consistency\n# split_point = 616131\n# X_train = X[:split_point]\n# y_train = y[:split_point]\n# X_val = X[split_point:] # Renamed X_test to X_val for clarity as it's a validation set\n# y_val = y[split_point:]\n\n\n# # Handle any remaining NaNs in numeric columns (after all feature engineering)\n# # LightGBM can handle NaNs, but it's often better to explicitly fill them\n# # using a strategy (e.g., median, mean, or 0) for consistency and to avoid unexpected behavior.\n# # Filling with 0 for simplicity, but consider the median for continuous features.\n# X_train = X_train.fillna(0)\n# X_val = X_val.fillna(0)\n# test_filtered = test_filtered.fillna(0)\n\n\n# # Compute scale_pos_weight for class imbalance\n# scale_pos_weight = (y == 0).sum() / (y == 1).sum()\n# print(f\"scale_pos_weight = {scale_pos_weight:.2f}\")\n\n# # Best params you recovered\n# params = {\n#     'learning_rate': 0.016623218477757856,\n#     'num_leaves': 88,\n#     'max_depth': 12,\n#     'min_child_samples': 31,\n#     'feature_fraction': 0.7857024999905345,\n#     'bagging_fraction': 0.8106542792647912,\n#     'bagging_freq': 2,\n#     'lambda_l1': 9.625481788587463,\n#     'lambda_l2': 4.969459532508245e-05,\n#     \"objective\": \"binary\",\n#     \"metric\": \"auc\",\n#     \"verbosity\": -1,\n#     \"boosting_type\": \"gbdt\",\n#     \"device\": \"gpu\",  # Remove this if no GPU is available\n#     \"scale_pos_weight\": scale_pos_weight, # Apply the calculated scale_pos_weight\n# }\n\n# # Train on the training split (X_train, y_train) for evaluating on X_val, y_val\n# train_data = lgb.Dataset(X_train, y_train)\n# val_data = lgb.Dataset(X_val, y_val, reference=train_data) # Use validation set for early stopping if desired\n\n# model = lgb.train(\n#     params,\n#     train_data,\n#     num_boost_round=1000,\n#     valid_sets=[val_data],\n#     callbacks=[lgb.early_stopping(100, verbose=False)] # Early stopping based on validation set\n# )\n\n# # Evaluate on validation set\n# preds_val = model.predict(X_val)\n# pr_auc_val = average_precision_score(y_val, preds_val)\n# print(f\"PR-AUC on validation set: {pr_auc_val:.4f}\")\n\n# # Re-train on the full dataset (X, y) with potentially more rounds (or use best_iteration from early stopping)\n# # This is often done for the final model to be saved for submission\n# print(\"\\nRetraining final model on full training data...\")\n# final_model = lgb.train(params, lgb.Dataset(X, y), num_boost_round=model.best_iteration) # Use best iteration from validation\n# final_model.save_model(\"lgbm_classweighted_optuna_pr_auc_new_features.txt\")\n# print(\"✅ Final Model retrained and saved.\")\n\n\n# # --- Get and Display New Feature Importance ---\n# # Use the final_model trained on the full dataset for importance\n# feat_imp_new = pd.DataFrame({\n#     'feature': final_model.feature_name(),\n#     'importance': final_model.feature_importance(importance_type='gain')\n# }).sort_values(by='importance', ascending=False)\n\n# print(\"\\n--- NEW Feature Importance (Top 50) ---\")\n# print(feat_imp_new.head(50))\n\n# # Optional: Visualize the new feature importance\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# plt.figure(figsize=(12, 18))\n# sns.barplot(x=\"importance\", y=\"feature\", data=feat_imp_new.head(50))\n# plt.title(\"LightGBM Feature Importance (Gain) with New Features\")\n# plt.xlabel(\"Feature Importance (Gain)\")\n# plt.ylabel(\"Feature Name\")\n# plt.tight_layout()\n# plt.show()\n\n\n# # --- Generate Predictions for Submission ---\n# # Use the 'test_filtered' DataFrame (which is the original test_features_final after all transformations)\n# prediction = final_model.predict(test_filtered)\n\n# # Add predictions to the original test_features_final (which still contains id1 for submission)\n# # Make sure df_test is the original test data with id1, id2, id3, id5 as used in your submission cell\n# # df_test = pd.read_csv('/kaggle/input/kanafrendatasetsplswork/test_features_final.csv')\n# # df_test_submission = df_test[['id1','id2','id3','id5']].copy() # Create a copy to avoid modifying original df_test\n# # df_test_submission['pred'] = prediction\n\n# # # Sort by id1 if needed for submission format\n# # # submission_df = submission_df.sort_values(by='id1')\n\n# # submission_df.to_csv('finalsubmission_with_new_features.csv', index=False)\n# # print(\"✅ Submission file created with new features.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T18:15:05.493297Z","iopub.status.idle":"2025-07-25T18:15:05.493605Z","shell.execute_reply.started":"2025-07-25T18:15:05.493448Z","shell.execute_reply":"2025-07-25T18:15:05.493462Z"}},"outputs":[],"execution_count":null}]}